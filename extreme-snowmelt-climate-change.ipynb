{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict extreme snowmelt events in climate change scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from collections import Counter\n",
    "import ast\n",
    "import dateutil.parser as parser\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV, KFold, cross_validate, PredefinedSplit\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, auc, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction.settings import MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train best model from previous analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 100/100 [00:38<00:00,  2.59it/s]\n",
      "Feature Extraction: 100%|██████████| 100/100 [00:59<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "## Recreate rolled dataframe with minimal features and undersample it (10 days, time lag zero)\n",
    "all_data_clean = pd.read_csv('../all_data_clean.csv')\n",
    "df_rolled = roll_time_series(\n",
    "    all_data_clean[['date', 'flow_site_id', 'flow', 'binary']], column_id=\"flow_site_id\", column_sort=\"date\", max_timeshift=10, min_timeshift=10 - 1, n_jobs=20)\n",
    "X_features_all = extract_features(\n",
    "\tdf_rolled.drop([\"binary\", \"flow_site_id\"], axis=1), column_id='id', column_sort='date',\n",
    "\tn_jobs=20, disable_progressbar=False, default_fc_parameters=MinimalFCParameters())\n",
    "X_features_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow__sum_values</th>\n",
       "      <th>flow__median</th>\n",
       "      <th>flow__mean</th>\n",
       "      <th>flow__length</th>\n",
       "      <th>flow__standard_deviation</th>\n",
       "      <th>flow__variance</th>\n",
       "      <th>flow__root_mean_square</th>\n",
       "      <th>flow__maximum</th>\n",
       "      <th>flow__minimum</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>binary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(114.0, 1909-01-10)</th>\n",
       "      <td>77.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.760000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>7.762087</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(114.0, 1909-01-10)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(114.0, 1909-01-11)</th>\n",
       "      <td>85.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.727273</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.200413</td>\n",
       "      <td>0.040165</td>\n",
       "      <td>7.729871</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(114.0, 1909-01-11)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(114.0, 1909-01-12)</th>\n",
       "      <td>84.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.672727</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.200413</td>\n",
       "      <td>0.040165</td>\n",
       "      <td>7.675344</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(114.0, 1909-01-12)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(114.0, 1909-01-13)</th>\n",
       "      <td>83.8</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.618182</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.184973</td>\n",
       "      <td>0.034215</td>\n",
       "      <td>7.620427</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(114.0, 1909-01-13)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(114.0, 1909-01-14)</th>\n",
       "      <td>83.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.563636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.149379</td>\n",
       "      <td>0.022314</td>\n",
       "      <td>7.565111</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(114.0, 1909-01-14)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     flow__sum_values  flow__median  flow__mean  flow__length  \\\n",
       "unique_id                                                                       \n",
       "(114.0, 1909-01-10)              77.6           7.7    7.760000          10.0   \n",
       "(114.0, 1909-01-11)              85.0           7.7    7.727273          11.0   \n",
       "(114.0, 1909-01-12)              84.4           7.7    7.672727          11.0   \n",
       "(114.0, 1909-01-13)              83.8           7.7    7.618182          11.0   \n",
       "(114.0, 1909-01-14)              83.2           7.7    7.563636          11.0   \n",
       "\n",
       "                     flow__standard_deviation  flow__variance  \\\n",
       "unique_id                                                       \n",
       "(114.0, 1909-01-10)                  0.180000        0.032400   \n",
       "(114.0, 1909-01-11)                  0.200413        0.040165   \n",
       "(114.0, 1909-01-12)                  0.200413        0.040165   \n",
       "(114.0, 1909-01-13)                  0.184973        0.034215   \n",
       "(114.0, 1909-01-14)                  0.149379        0.022314   \n",
       "\n",
       "                     flow__root_mean_square  flow__maximum  flow__minimum  \\\n",
       "unique_id                                                                   \n",
       "(114.0, 1909-01-10)                7.762087            8.0            7.4   \n",
       "(114.0, 1909-01-11)                7.729871            8.0            7.4   \n",
       "(114.0, 1909-01-12)                7.675344            8.0            7.4   \n",
       "(114.0, 1909-01-13)                7.620427            8.0            7.4   \n",
       "(114.0, 1909-01-14)                7.565111            7.7            7.4   \n",
       "\n",
       "                               unique_id  binary  \n",
       "unique_id                                         \n",
       "(114.0, 1909-01-10)  (114.0, 1909-01-10)       0  \n",
       "(114.0, 1909-01-11)  (114.0, 1909-01-11)       0  \n",
       "(114.0, 1909-01-12)  (114.0, 1909-01-12)       0  \n",
       "(114.0, 1909-01-13)  (114.0, 1909-01-13)       0  \n",
       "(114.0, 1909-01-14)  (114.0, 1909-01-14)       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features_all['unique_id'] = X_features_all.index\n",
    "\n",
    "all_data_clean['shifted_date'] = pd.to_datetime(\n",
    "    all_data_clean.date) + pd.Timedelta(days=0)\n",
    "all_data_clean['shifted_date'] = all_data_clean['shifted_date'].dt.strftime(\n",
    "    '%Y-%m-%d')\n",
    "all_data_clean['unique_id'] = list(\n",
    "    zip(all_data_clean.flow_site_id, all_data_clean.shifted_date))\n",
    "all_data_clean = all_data_clean.dropna()\n",
    "\n",
    "X_features_all = X_features_all.reset_index(drop=True)\n",
    "\n",
    "X_features_all = pd.merge(X_features_all, all_data_clean[[\n",
    "                          'binary', 'unique_id']], how='left', on='unique_id')\n",
    "X_features_all = X_features_all.set_index(\n",
    "    X_features_all['unique_id'], drop=True)\n",
    "X_features_all = X_features_all.dropna()\n",
    "X_features_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5516, 1: 5516})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## only keep 'flow__sum_values', 'flow__standard_deviation', 'flow__variance', 'flow__minimum'\n",
    "y1 = X_features_all['binary']\n",
    "undersample = NearMiss(version=3, n_neighbors=3)\n",
    "X_under, y_under = undersample.fit_resample(X_features_all.drop(\n",
    "    columns=['binary', 'unique_id', 'flow__median', 'flow__mean', 'flow__length', 'flow__root_mean_square', 'flow__maximum']), y1)\n",
    "X_under.index = X_features_all['unique_id'][undersample.sample_indices_]\n",
    "y_under.index = X_features_all['unique_id'][undersample.sample_indices_]\n",
    "Counter(y_under)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fit random forest model with parameters {'max_depth': 1, 'max_features': 2, 'n_estimators': 500} \n",
    "\n",
    "clf = RandomForestClassifier(max_depth= 1, max_features=2, n_estimators=500, n_jobs=-1, random_state=42, verbose=0)\n",
    "clf.fit(X_under, y_under)\n",
    "present_all = clf.predict(X_under)\n",
    "present_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create scenario data and predict extreme snowmelt for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow__sum_values</th>\n",
       "      <th>flow__standard_deviation</th>\n",
       "      <th>flow__variance</th>\n",
       "      <th>flow__minimum</th>\n",
       "      <th>site_id</th>\n",
       "      <th>rcp26_rang</th>\n",
       "      <th>rcp45_rang</th>\n",
       "      <th>rcp85_rang</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1315.0, 1959-10-28)</th>\n",
       "      <td>4240.000</td>\n",
       "      <td>329.537517</td>\n",
       "      <td>108594.975207</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>7.80</td>\n",
       "      <td>13.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2012.0, 1992-06-06)</th>\n",
       "      <td>5011.897</td>\n",
       "      <td>325.292310</td>\n",
       "      <td>105815.087258</td>\n",
       "      <td>64.9905</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>7.80</td>\n",
       "      <td>13.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1315.0, 1959-10-31)</th>\n",
       "      <td>4267.000</td>\n",
       "      <td>327.285379</td>\n",
       "      <td>107115.719008</td>\n",
       "      <td>95.0000</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>7.80</td>\n",
       "      <td>13.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1909.0, 1967-10-28)</th>\n",
       "      <td>2021.000</td>\n",
       "      <td>233.188215</td>\n",
       "      <td>54376.743802</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>3.38</td>\n",
       "      <td>7.69</td>\n",
       "      <td>13.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1909.0, 1967-10-24)</th>\n",
       "      <td>3710.000</td>\n",
       "      <td>230.753348</td>\n",
       "      <td>53247.107438</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>3.38</td>\n",
       "      <td>7.69</td>\n",
       "      <td>13.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2372.0, 1995-02-03)</th>\n",
       "      <td>371.000</td>\n",
       "      <td>7.886593</td>\n",
       "      <td>62.198347</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>-4.16</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2372.0, 1995-02-04)</th>\n",
       "      <td>383.000</td>\n",
       "      <td>8.515421</td>\n",
       "      <td>72.512397</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>-4.16</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2372.0, 1996-01-11)</th>\n",
       "      <td>38.100</td>\n",
       "      <td>0.308288</td>\n",
       "      <td>0.095041</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>-4.16</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2372.0, 1998-02-05)</th>\n",
       "      <td>111.900</td>\n",
       "      <td>2.383310</td>\n",
       "      <td>5.680165</td>\n",
       "      <td>6.3000</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>-4.16</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2372.0, 1998-02-06)</th>\n",
       "      <td>123.400</td>\n",
       "      <td>2.917601</td>\n",
       "      <td>8.512397</td>\n",
       "      <td>7.2000</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>-4.16</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-1.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10777 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      flow__sum_values  flow__standard_deviation  \\\n",
       "unique_id                                                          \n",
       "(1315.0, 1959-10-28)          4240.000                329.537517   \n",
       "(2012.0, 1992-06-06)          5011.897                325.292310   \n",
       "(1315.0, 1959-10-31)          4267.000                327.285379   \n",
       "(1909.0, 1967-10-28)          2021.000                233.188215   \n",
       "(1909.0, 1967-10-24)          3710.000                230.753348   \n",
       "...                                ...                       ...   \n",
       "(2372.0, 1995-02-03)           371.000                  7.886593   \n",
       "(2372.0, 1995-02-04)           383.000                  8.515421   \n",
       "(2372.0, 1996-01-11)            38.100                  0.308288   \n",
       "(2372.0, 1998-02-05)           111.900                  2.383310   \n",
       "(2372.0, 1998-02-06)           123.400                  2.917601   \n",
       "\n",
       "                      flow__variance  flow__minimum  site_id  rcp26_rang  \\\n",
       "unique_id                                                                  \n",
       "(1315.0, 1959-10-28)   108594.975207        74.0000   1315.0        3.08   \n",
       "(2012.0, 1992-06-06)   105815.087258        64.9905   2012.0        3.08   \n",
       "(1315.0, 1959-10-31)   107115.719008        95.0000   1315.0        3.08   \n",
       "(1909.0, 1967-10-28)    54376.743802        41.0000   1909.0        3.38   \n",
       "(1909.0, 1967-10-24)    53247.107438        41.0000   1909.0        3.38   \n",
       "...                              ...            ...      ...         ...   \n",
       "(2372.0, 1995-02-03)       62.198347        26.0000   2372.0       -4.16   \n",
       "(2372.0, 1995-02-04)       72.512397        26.0000   2372.0       -4.16   \n",
       "(2372.0, 1996-01-11)        0.095041         3.0000   2372.0       -4.16   \n",
       "(2372.0, 1998-02-05)        5.680165         6.3000   2372.0       -4.16   \n",
       "(2372.0, 1998-02-06)        8.512397         7.2000   2372.0       -4.16   \n",
       "\n",
       "                      rcp45_rang  rcp85_rang  \n",
       "unique_id                                     \n",
       "(1315.0, 1959-10-28)        7.80       13.65  \n",
       "(2012.0, 1992-06-06)        7.80       13.65  \n",
       "(1315.0, 1959-10-31)        7.80       13.65  \n",
       "(1909.0, 1967-10-28)        7.69       13.49  \n",
       "(1909.0, 1967-10-24)        7.69       13.49  \n",
       "...                          ...         ...  \n",
       "(2372.0, 1995-02-03)       -1.04       -1.84  \n",
       "(2372.0, 1995-02-04)       -1.04       -1.84  \n",
       "(2372.0, 1996-01-11)       -1.04       -1.84  \n",
       "(2372.0, 1998-02-05)       -1.04       -1.84  \n",
       "(2372.0, 1998-02-06)       -1.04       -1.84  \n",
       "\n",
       "[10777 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## merge scenario ranges with data based on site id\n",
    "scenarios = gpd.read_file('../flow_sites_scenarios/flow_sites_scenarios.shp')\n",
    "X_under[['site_id', 'date']] = X_under.index.to_list()\n",
    "scenarios['site_id'] = scenarios['site_id'].astype(float)\n",
    "X_under_scenarios = pd.merge(X_under.drop(columns=['date']), scenarios[['site_id', 'rcp26_rang', 'rcp45_rang', 'rcp85_rang']], how='left', on='site_id')\n",
    "X_under_scenarios.index = X_under.index\n",
    "X_under_scenarios.dropna(inplace=True)\n",
    "X_under_scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5408, 1: 5369})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_under = y_under[y_under.index.isin(X_under_scenarios.index)]\n",
    "Counter(y_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flow__sum_values            0\n",
       "flow__standard_deviation    0\n",
       "flow__variance              0\n",
       "flow__minimum               0\n",
       "site_id                     0\n",
       "rcp26_rang                  0\n",
       "rcp45_rang                  0\n",
       "rcp85_rang                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_under_scenarios.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict extreme snowmelt for all data after changing sum_values and minimum by range value (for all 3 scenarios)\n",
    "X_under_scenarios_26 = X_under_scenarios.drop(columns=['rcp45_rang', 'rcp85_rang', 'site_id'])\n",
    "X_under_scenarios_26['flow__sum_values'] = X_under_scenarios_26['flow__sum_values'] + (X_under_scenarios_26['flow__sum_values'] * X_under_scenarios_26['rcp26_rang'] / 100)\n",
    "X_under_scenarios_26['flow__minimum'] = X_under_scenarios_26['flow__minimum'] + (X_under_scenarios_26['flow__minimum'] * X_under_scenarios_26['rcp26_rang'] / 100)\n",
    "X_under_scenarios_26.drop(columns=['rcp26_rang'], inplace=True)\n",
    "\n",
    "X_under_scenarios_45 = X_under_scenarios.drop(columns=['rcp26_rang', 'rcp85_rang', 'site_id'])\n",
    "X_under_scenarios_45['flow__sum_values'] = X_under_scenarios_45['flow__sum_values'] + (X_under_scenarios_45['flow__sum_values'] * X_under_scenarios_45['rcp45_rang'] / 100)\n",
    "X_under_scenarios_45['flow__minimum'] = X_under_scenarios_45['flow__minimum'] + (X_under_scenarios_45['flow__minimum'] * X_under_scenarios_45['rcp45_rang'] / 100)\n",
    "X_under_scenarios_45.drop(columns=['rcp45_rang'], inplace=True)\n",
    "\n",
    "X_under_scenarios_85 = X_under_scenarios.drop(columns=['rcp26_rang', 'rcp45_rang', 'site_id'])\n",
    "X_under_scenarios_85['flow__sum_values'] = X_under_scenarios_85['flow__sum_values'] + (X_under_scenarios_85['flow__sum_values'] * X_under_scenarios_85['rcp85_rang'] / 100)\n",
    "X_under_scenarios_85['flow__minimum'] = X_under_scenarios_85['flow__minimum'] + (X_under_scenarios_85['flow__minimum'] * X_under_scenarios_85['rcp85_rang'] / 100)\n",
    "X_under_scenarios_85.drop(columns=['rcp85_rang'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcp26_all = clf.predict(X_under_scenarios_26)\n",
    "rcp45_all = clf.predict(X_under_scenarios_45)\n",
    "rcp85_all = clf.predict(X_under_scenarios_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 7349, 1: 3428})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(rcp26_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data outside training range and predict again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate min and sum_values for original training data and remove scenario data that is not within the range\n",
    "\n",
    "\n",
    "## predict extreme snowmelt with reduced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results of different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare present, all-data scenarios and separately cleaned data present and scenarios"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
