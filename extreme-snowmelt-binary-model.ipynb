{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model - flow vs binary extreme snowmelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for search grid\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "class loguniform_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = loguniform(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build scorer function\n",
    "def auc_pr_score(y_true, y_pred):\n",
    "\tprecision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "\treturn auc(recall, precision)\n",
    "\n",
    "\n",
    "auc_pr = make_scorer(auc_pr_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_clean = pd.read_csv('../all_data_clean.csv')\n",
    "\n",
    "all_data_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training, test, validation data\n",
    "scores = {}\n",
    "n_splits = 5\n",
    "\n",
    "all_data_clean = all_data_clean.sort_values(by=['date'], ascending=True).reset_index(drop=True)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "months_df = pd.DataFrame(enc.fit_transform(\n",
    "    all_data_clean[['month']]).toarray())\n",
    "#X = all_data_clean[['flow']].join(months_df)\n",
    "X = all_data_clean[['date', 'site_id', 'flow']]\n",
    "y = all_data_clean['binary']\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model: naive predictor (always zero)\n",
    "y_pred = np.zeros(y.shape)\n",
    "auc_score = auc_pr_score(y, y_pred)\n",
    "scores['baseline'] = auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a test with logistic regression\n",
    "\n",
    "tmp_scores = []\n",
    "for train_index, test_index in tscv.split(X):\n",
    "\n",
    "    data_train = X.loc[X.index.intersection(train_index), :]\n",
    "    target_train = y.loc[y.index.intersection(train_index)]\n",
    "\n",
    "    data_test = X.loc[X.index.intersection(test_index), :]\n",
    "    target_test = y.loc[y.index.intersection(test_index)]\n",
    "\n",
    "    clf = LogisticRegression(class_weight={0: 1, 1: 10})\n",
    "    clf.fit(data_train, target_train)\n",
    "\n",
    "    preds = clf.predict(data_test)\n",
    "    y_pred = pd.Series(\n",
    "        np.round(preds), index=data_test.index, name='predicted')\n",
    "    gridsearchpreds = pd.concat([target_test, y_pred], axis=1)\n",
    "    print(gridsearchpreds.value_counts())\n",
    "\n",
    "    auc_score = auc_pr_score(target_test, preds)\n",
    "    print(auc_score)\n",
    "\n",
    "    tmp_scores.append(auc_score)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_score = np.mean(tmp_scores)\n",
    "scores['logistic regression'] = average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM-like ensemble model - parameters tuned for this dataset\n",
    "\n",
    "index_output = tscv.split(X)\n",
    "\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    loss='binary_crossentropy', random_state=42, verbose=0).fit(X, y)\n",
    "param_grid = {\n",
    "    'learning_rate': (0.1, 1, 5, 7, 10, 20, 50, 100),\n",
    "   \t#'max_iter': (100, 200, 300, 500, 1000),\n",
    "    'max_leaf_nodes': (1, 2, 3, 5, 10, 30)}\n",
    "\n",
    "# consider using RandomizedSearchCV instead of GridSearchCV\n",
    "# gs = GridSearchCV(clf, param_grid=param_grid, cv=index_output, scoring=auc_pr, n_jobs=-1, verbose=1)\n",
    "# gs.fit(X, y)\n",
    "# gs.best_params_\n",
    "\n",
    "param_distributions = {\n",
    "    'l2_regularization': loguniform(1e-6, 1e3),\n",
    "    'learning_rate': loguniform(0.001, 10),\n",
    "    'max_leaf_nodes': loguniform_int(2, 256),\n",
    "    'min_samples_leaf': loguniform_int(1, 100),\n",
    "    'max_bins': loguniform_int(2, 255),\n",
    "}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    clf, param_distributions=param_distributions, n_iter=200, n_jobs=-1, scoring=auc_pr,\n",
    "    cv=index_output, verbose=1,\n",
    ")\n",
    "model_random_search.fit(X, y)\n",
    "l2_regularization = model_random_search.best_params_['l2_regularization']\n",
    "learning_rate = model_random_search.best_params_['learning_rate']\n",
    "max_leaf_nodes = model_random_search.best_params_['max_leaf_nodes']\n",
    "min_samples_leaf = model_random_search.best_params_['min_samples_leaf']\n",
    "max_bins = model_random_search.best_params_['max_bins']\n",
    "model_random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM-like ensemble model - cross validation with new parameters\n",
    "\n",
    "new_regressor = HistGradientBoostingClassifier(\n",
    "    loss='binary_crossentropy', random_state=42, l2_regularization=131.12520655871074,\n",
    "    learning_rate=learning_rate,\n",
    "    max_bins=max_bins,\n",
    "    max_leaf_nodes=max_leaf_nodes,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "   \tverbose=0)\n",
    "index_output = tscv.split(X)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    new_regressor,\n",
    "    X,\n",
    "    y,\n",
    "    cv=index_output,\n",
    "    scoring=auc_pr,\n",
    ")\n",
    "\n",
    "mean_score = np.mean(cv_results['test_score'])\n",
    "scores['histogram gradient boosting'] = mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model predictions\n",
    "y_pred = new_regressor.fit(X, y).predict(X)\n",
    "y_pred = pd.Series(np.round(y_pred), index=X.index, name='predicted')\n",
    "np.unique(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at confusion matrix\n",
    "gridsearchpreds = pd.concat([y, y_pred], axis=1)\n",
    "gridsearchpreds.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "index_output = tscv.split(X)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, verbose=0)\n",
    "\n",
    "param_distributions = {\n",
    "\t'n_estimators': loguniform_int(2, 256),\n",
    "\t'criterion': ['gini', 'entropy'],\n",
    "\t'min_samples_split': loguniform_int(2, 256),\n",
    "\t'min_samples_leaf': loguniform_int(1, 100),\n",
    "\t'class_weight': [{0: 1, 1: 10}, {0: 1, 1: 5}, {0: 1, 1: 2}, {0: 1, 1: 1}],\n",
    "}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    clf, param_distributions=param_distributions, n_iter=200, n_jobs=-1, scoring=auc_pr,\n",
    "    cv=index_output, verbose=1,\n",
    ")\n",
    "model_random_search.fit(X, y)\n",
    "n_estimators = model_random_search.best_params_['n_estimators']\n",
    "criterion = model_random_search.best_params_['criterion']\n",
    "min_samples_split = model_random_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = model_random_search.best_params_['min_samples_leaf']\n",
    "class_weight = model_random_search.best_params_['class_weight']\n",
    "model_random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest - cross validation with new parameters\n",
    "\n",
    "new_regressor = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "   \tn_jobs=-1, verbose=0,\n",
    "   \tcriterion=criterion,\n",
    "   \tmin_samples_leaf=min_samples_leaf,\n",
    "   \tmin_samples_split=min_samples_split,\n",
    "   \tn_estimators=n_estimators\n",
    ")\n",
    "index_output = tscv.split(X)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    new_regressor,\n",
    "    X,\n",
    "    y,\n",
    "    cv=index_output,\n",
    "    scoring=auc_pr,\n",
    ")\n",
    "\n",
    "mean_score = np.mean(cv_results['test_score'])\n",
    "scores['random forest'] = mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model predictions\n",
    "y_pred = new_regressor.fit(X, y).predict(X)\n",
    "y_pred = pd.Series(np.round(y_pred), index=X.index, name='predicted')\n",
    "np.unique(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at confusion matrix\n",
    "gridsearchpreds = pd.concat([y, y_pred], axis=1)\n",
    "gridsearchpreds.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with RNN \n",
    "\n",
    "values = all_data_clean[['flow', 'flow_prev_month', 'binary']].values\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "df = pd.DataFrame(scaled, columns=['flow', 'flow_prev_month', 'binary'])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = df.values\n",
    "n_train_hours = int(np.round(len(df) * 0.8))\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd')  # optimizers: adam, sgd\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=10, batch_size=72,\n",
    "                    validation_data=(test_X, test_y), verbose=2, shuffle=False, workers=20)\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_pred = model.predict(test_X).flatten()\n",
    "y_pred = pd.Series(np.round(y_pred), name='predicted')\n",
    "np.unique(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at confusion matrix\n",
    "test_y = pd.Series(test_y, name='actual')\n",
    "score = auc_pr(test_y, y_pred)\n",
    "scores['RNN'] = score\n",
    "gridsearchpreds = pd.concat([test_y, y_pred], axis=1)\n",
    "gridsearchpreds.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all AUC-PR\n",
    "for key, value in scores.items():\n",
    "\tprint(key, value)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
