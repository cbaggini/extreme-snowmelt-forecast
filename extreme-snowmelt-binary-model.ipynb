{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model - flow vs binary extreme snowmelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for search grid\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "class loguniform_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = loguniform(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build scorer function\n",
    "def auc_pr_score(y_true, y_pred):\n",
    "\tprecision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "\treturn auc(recall, precision)\n",
    "\n",
    "\n",
    "auc_pr = make_scorer(auc_pr_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>depth_diff</th>\n",
       "      <th>binary</th>\n",
       "      <th>flow_site_id</th>\n",
       "      <th>snow_site_id</th>\n",
       "      <th>distance</th>\n",
       "      <th>flow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>flow_prev_month</th>\n",
       "      <th>flow_prev_month_max</th>\n",
       "      <th>site_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1945-05-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>157010.0</td>\n",
       "      <td>35296.19718</td>\n",
       "      <td>182.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>5</td>\n",
       "      <td>413.3518</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>51.0 157010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1945-05-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>157010.0</td>\n",
       "      <td>35296.19718</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>5</td>\n",
       "      <td>413.3518</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>51.0 157010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1945-05-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>157010.0</td>\n",
       "      <td>35296.19718</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>5</td>\n",
       "      <td>413.3518</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>51.0 157010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1945-05-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>157010.0</td>\n",
       "      <td>35296.19718</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>5</td>\n",
       "      <td>413.3518</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>51.0 157010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1945-05-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>157010.0</td>\n",
       "      <td>35296.19718</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>5</td>\n",
       "      <td>413.3518</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>51.0 157010.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date  snow_depth  depth_diff  binary  flow_site_id  \\\n",
       "0           0  1945-05-23         0.0         0.0       0          51.0   \n",
       "1           1  1945-05-24         0.0         0.0       0          51.0   \n",
       "2           2  1945-05-25         0.0         0.0       0          51.0   \n",
       "3           3  1945-05-26         0.0         0.0       0          51.0   \n",
       "4           4  1945-05-27         0.0         0.0       0          51.0   \n",
       "\n",
       "   snow_site_id     distance   flow  year  month  flow_prev_month  \\\n",
       "0      157010.0  35296.19718  182.0  1945      5         413.3518   \n",
       "1      157010.0  35296.19718  176.0  1945      5         413.3518   \n",
       "2      157010.0  35296.19718  164.0  1945      5         413.3518   \n",
       "3      157010.0  35296.19718  152.0  1945      5         413.3518   \n",
       "4      157010.0  35296.19718  143.0  1945      5         413.3518   \n",
       "\n",
       "   flow_prev_month_max        site_id  \n",
       "0               1746.0  51.0 157010.0  \n",
       "1               1746.0  51.0 157010.0  \n",
       "2               1746.0  51.0 157010.0  \n",
       "3               1746.0  51.0 157010.0  \n",
       "4               1746.0  51.0 157010.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_clean = pd.read_csv('../all_data_clean.csv')\n",
    "\n",
    "all_data_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training, test, validation data\n",
    "scores = {}\n",
    "n_splits = 5\n",
    "\n",
    "all_data_clean = all_data_clean.sort_values(by=['date'], ascending=True).reset_index(drop=True)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "months_df = pd.DataFrame(enc.fit_transform(\n",
    "    all_data_clean[['month']]).toarray())\n",
    "#X = all_data_clean[['flow']].join(months_df)\n",
    "X = all_data_clean[['flow', 'month', 'year']]\n",
    "y = all_data_clean['binary']\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model: naive predictor (always zero)\n",
    "y_pred = np.zeros(y.shape)\n",
    "auc_score = auc_pr_score(y, y_pred)\n",
    "scores['baseline'] = auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary  predicted\n",
      "0       0            228513\n",
      "1       0             21066\n",
      "0       1              1376\n",
      "1       1                87\n",
      "dtype: int64\n",
      "0.07374699307700014\n",
      "binary  predicted\n",
      "0       0            201825\n",
      "        1             34617\n",
      "1       0             12469\n",
      "        1              2131\n",
      "dtype: int64\n",
      "0.12680871712697767\n",
      "binary  predicted\n",
      "0       0            198617\n",
      "        1             35486\n",
      "1       0             14220\n",
      "        1              2719\n",
      "dtype: int64\n",
      "0.14416487658016947\n",
      "binary  predicted\n",
      "0       0            208398\n",
      "        1             32104\n",
      "1       0              8805\n",
      "        1              1735\n",
      "dtype: int64\n",
      "0.12547850937893773\n",
      "binary  predicted\n",
      "0       0            217463\n",
      "        1             25542\n",
      "1       0              6653\n",
      "        1              1384\n",
      "dtype: int64\n",
      "0.12505261690811537\n"
     ]
    }
   ],
   "source": [
    "# a test with logistic regression\n",
    "\n",
    "tmp_scores = []\n",
    "for train_index, test_index in tscv.split(X):\n",
    "\n",
    "    data_train = X.loc[X.index.intersection(train_index), :]\n",
    "    target_train = y.loc[y.index.intersection(train_index)]\n",
    "\n",
    "    data_test = X.loc[X.index.intersection(test_index), :]\n",
    "    target_test = y.loc[y.index.intersection(test_index)]\n",
    "\n",
    "    clf = LogisticRegression(class_weight={0: 1, 1: 10})\n",
    "    clf.fit(data_train, target_train)\n",
    "\n",
    "    preds = clf.predict(data_test)\n",
    "    y_pred = pd.Series(\n",
    "        np.round(preds), index=data_test.index, name='predicted')\n",
    "    gridsearchpreds = pd.concat([target_test, y_pred], axis=1)\n",
    "    print(gridsearchpreds.value_counts())\n",
    "\n",
    "    auc_score = auc_pr_score(target_test, preds)\n",
    "    print(auc_score)\n",
    "\n",
    "    tmp_scores.append(auc_score)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_score = np.mean(tmp_scores)\n",
    "scores['logistic regression'] = average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l2_regularization': 0.04370362167050531,\n",
       " 'learning_rate': 0.03132616481962874,\n",
       " 'max_bins': 7,\n",
       " 'max_leaf_nodes': 54,\n",
       " 'min_samples_leaf': 16}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LightGBM-like ensemble model - parameters tuned for this dataset\n",
    "\n",
    "index_output = tscv.split(X)\n",
    "\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    loss='binary_crossentropy', random_state=42, verbose=0).fit(X, y)\n",
    "param_grid = {\n",
    "    'learning_rate': (0.1, 1, 5, 7, 10, 20, 50, 100),\n",
    "   \t#'max_iter': (100, 200, 300, 500, 1000),\n",
    "    'max_leaf_nodes': (1, 2, 3, 5, 10, 30)}\n",
    "\n",
    "# consider using RandomizedSearchCV instead of GridSearchCV\n",
    "# gs = GridSearchCV(clf, param_grid=param_grid, cv=index_output, scoring=auc_pr, n_jobs=-1, verbose=1)\n",
    "# gs.fit(X, y)\n",
    "# gs.best_params_\n",
    "\n",
    "param_distributions = {\n",
    "    'l2_regularization': loguniform(1e-6, 1e3),\n",
    "    'learning_rate': loguniform(0.001, 10),\n",
    "    'max_leaf_nodes': loguniform_int(2, 256),\n",
    "    'min_samples_leaf': loguniform_int(1, 100),\n",
    "    'max_bins': loguniform_int(2, 255),\n",
    "}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    clf, param_distributions=param_distributions, n_iter=200, n_jobs=-1, scoring=auc_pr,\n",
    "    cv=index_output, verbose=1,\n",
    ")\n",
    "model_random_search.fit(X, y)\n",
    "l2_regularization = model_random_search.best_params_['l2_regularization']\n",
    "learning_rate = model_random_search.best_params_['learning_rate']\n",
    "max_leaf_nodes = model_random_search.best_params_['max_leaf_nodes']\n",
    "min_samples_leaf = model_random_search.best_params_['min_samples_leaf']\n",
    "max_bins = model_random_search.best_params_['max_bins']\n",
    "model_random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM-like ensemble model - cross validation with new parameters\n",
    "\n",
    "new_regressor = HistGradientBoostingClassifier(\n",
    "    loss='binary_crossentropy', random_state=42, l2_regularization=131.12520655871074,\n",
    "    learning_rate=learning_rate,\n",
    "    max_bins=max_bins,\n",
    "    max_leaf_nodes=max_leaf_nodes,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "   \tverbose=0)\n",
    "index_output = tscv.split(X)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    new_regressor,\n",
    "    X,\n",
    "    y,\n",
    "    cv=index_output,\n",
    "    scoring=auc_pr,\n",
    ")\n",
    "\n",
    "mean_score = np.mean(cv_results['test_score'])\n",
    "scores['histogram gradient boosting'] = mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model predictions\n",
    "y_pred = new_regressor.fit(X, y).predict(X)\n",
    "y_pred = pd.Series(np.round(y_pred), index=X.index, name='predicted')\n",
    "np.unique(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binary  predicted\n",
       "0       0            1420814\n",
       "1       0              85438\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at confusion matrix\n",
    "gridsearchpreds = pd.concat([y, y_pred], axis=1)\n",
    "gridsearchpreds.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_weight': {0: 1, 1: 1},\n",
       " 'criterion': 'gini',\n",
       " 'min_samples_leaf': 96,\n",
       " 'min_samples_split': 50,\n",
       " 'n_estimators': 8}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "index_output = tscv.split(X)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, verbose=0)\n",
    "\n",
    "param_distributions = {\n",
    "\t'n_estimators': loguniform_int(2, 256),\n",
    "\t'criterion': ['gini', 'entropy'],\n",
    "\t'min_samples_split': loguniform_int(2, 256),\n",
    "\t'min_samples_leaf': loguniform_int(1, 100),\n",
    "\t'class_weight': [{0: 1, 1: 10}, {0: 1, 1: 5}, {0: 1, 1: 2}, {0: 1, 1: 1}],\n",
    "}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    clf, param_distributions=param_distributions, n_iter=200, n_jobs=-1, scoring=auc_pr,\n",
    "    cv=index_output, verbose=1,\n",
    ")\n",
    "model_random_search.fit(X, y)\n",
    "n_estimators = model_random_search.best_params_['n_estimators']\n",
    "criterion = model_random_search.best_params_['criterion']\n",
    "min_samples_split = model_random_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = model_random_search.best_params_['min_samples_leaf']\n",
    "class_weight = model_random_search.best_params_['class_weight']\n",
    "model_random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest - cross validation with new parameters\n",
    "\n",
    "new_regressor = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "   \tn_jobs=-1, verbose=0,\n",
    "   \tcriterion=criterion,\n",
    "   \tmin_samples_leaf=min_samples_leaf,\n",
    "   \tmin_samples_split=min_samples_split,\n",
    "   \tn_estimators=n_estimators\n",
    ")\n",
    "index_output = tscv.split(X)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    new_regressor,\n",
    "    X,\n",
    "    y,\n",
    "    cv=index_output,\n",
    "    scoring=auc_pr,\n",
    ")\n",
    "\n",
    "mean_score = np.mean(cv_results['test_score'])\n",
    "scores['random forest'] = mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model predictions\n",
    "y_pred = new_regressor.fit(X, y).predict(X)\n",
    "y_pred = pd.Series(np.round(y_pred), index=X.index, name='predicted')\n",
    "np.unique(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binary  predicted\n",
       "0       0            1420332\n",
       "1       0              84555\n",
       "        1                883\n",
       "0       1                482\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at confusion matrix\n",
    "gridsearchpreds = pd.concat([y, y_pred], axis=1)\n",
    "gridsearchpreds.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       flow  flow_prev_month  binary\n",
      "0  0.027101         0.053196     0.0\n",
      "1  0.024271         0.053196     0.0\n",
      "2  0.023956         0.053196     0.0\n",
      "3  0.023641         0.053196     0.0\n",
      "4  0.023327         0.038382     0.0\n"
     ]
    }
   ],
   "source": [
    "# try with RNN \n",
    "\n",
    "values = all_data_clean[['flow', 'flow_prev_month', 'binary']].values\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "df = pd.DataFrame(scaled, columns=['flow', 'flow_prev_month', 'binary'])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1205002, 1, 2) (1205002,) (301250, 1, 2) (301250,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "values = df.values\n",
    "n_train_hours = int(np.round(len(df) * 0.8))\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16737/16737 - 50s - loss: 0.9742 - val_loss: 0.4777 - 50s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "16737/16737 - 48s - loss: 0.9742 - val_loss: 0.4777 - 48s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "16737/16737 - 47s - loss: 0.9742 - val_loss: 0.4777 - 47s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "16737/16737 - 48s - loss: 0.9742 - val_loss: 0.4777 - 48s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "16737/16737 - 48s - loss: 0.9742 - val_loss: 0.4777 - 48s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "16737/16737 - 48s - loss: 0.9742 - val_loss: 0.4777 - 48s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "16737/16737 - 48s - loss: 0.9742 - val_loss: 0.4777 - 48s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "16737/16737 - 48s - loss: 0.9742 - val_loss: 0.4777 - 48s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "16737/16737 - 47s - loss: 0.9742 - val_loss: 0.4777 - 47s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "16737/16737 - 48s - loss: 0.9742 - val_loss: 0.4777 - 48s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd')  # optimizers: adam, sgd\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=10, batch_size=72,\n",
    "                    validation_data=(test_X, test_y), verbose=2, shuffle=False, workers=20)\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a prediction\n",
    "y_pred = model.predict(test_X).flatten()\n",
    "y_pred = pd.Series(np.round(y_pred), name='predicted')\n",
    "np.unique(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual  predicted\n",
       "0.0     -0.0         291920\n",
       "1.0     -0.0           9330\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at confusion matrix\n",
    "test_y = pd.Series(test_y, name='actual')\n",
    "score = auc_pr_score(test_y, y_pred)\n",
    "scores['RNN'] = score\n",
    "gridsearchpreds = pd.concat([test_y, y_pred], axis=1)\n",
    "gridsearchpreds.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 0.5283611241678019\n",
      "logistic regression 0.11905034261424008\n",
      "histogram gradient boosting 0.5283892735080186\n",
      "random forest 0.5283892735080186\n",
      "RNN 0.5154854771784232\n"
     ]
    }
   ],
   "source": [
    "# all AUC-PR\n",
    "for key, value in scores.items():\n",
    "\tprint(key, value)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
