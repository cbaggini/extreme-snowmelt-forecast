{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract time series parameters from flow and use to predict extreme snowmelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import ast\n",
    "import dateutil.parser as parser\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV, KFold, cross_validate, PredefinedSplit\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, auc, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction.settings import MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build scorer function\n",
    "def auc_pr_score(y_true, y_pred):\n",
    "\tprecision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "\treturn auc(recall, precision)\n",
    "\n",
    "\n",
    "auc_pr = make_scorer(auc_pr_score, greater_is_better=True)\n",
    "\n",
    "N_DAYS = 5\n",
    "TIME_LAG = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define functions\n",
    "\n",
    "def random_forest_site(X_under, y_under, results, options):\n",
    "\tindx = X_under.index.unique()\n",
    "\tsites = [x[0] for x in indx]\n",
    "\tsites = list(set(sites))\n",
    "\tsites_train = sites[:len(sites)//10*6]\n",
    "\tsites_test = sites[len(sites)//10*6:]\n",
    "\tidx_train = [x for x in indx if x[0] in sites_train]\n",
    "\tidx_test = [x for x in indx if x[0] in sites_test]\n",
    "\tX_filtered_train = X_under[X_under.index.isin(idx_train)]\n",
    "\tX_filtered_test = X_under[X_under.index.isin(idx_test)]\n",
    "\ty_train = y_under[y_under.index.isin(idx_train)]\n",
    "\ty_test = y_under[y_under.index.isin(idx_test)]\n",
    "\n",
    "\tX_all = pd.concat([X_filtered_train, X_filtered_test]).reset_index(drop=True)\n",
    "\ty_all = pd.concat([y_train, y_test]).reset_index(drop=True)\n",
    "\tclf = RandomForestClassifier(n_jobs=-1, random_state=42, verbose=0)\n",
    "\n",
    "\tsplit_index = [-1 if x in X_filtered_train.index else 0 for x in X_under.index]\n",
    "\tps = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "\tparam_grid = {\n",
    "\t\t'max_depth': (1, 5, 10, 25),\n",
    "\t\t'n_estimators': (100, 500, 750, 1500),\n",
    "\t\t'max_features': (2, 3, 5, 10)}\n",
    "\n",
    "\tgs = GridSearchCV(clf, param_grid=param_grid, cv=ps,\n",
    "\t\t\t\t\tscoring=auc_pr, n_jobs=-1, verbose=0)\n",
    "\tgs.fit(X_all, y_all)\n",
    "\tprint(gs.best_params_, gs.best_score_)\n",
    "\tresults = results.append({'n_days': N_DAYS, 'time_lag': TIME_LAG, 'eval_type': 'site', 'param_type': options['param_type'], 'year': options['year'],\n",
    "                           'params': gs.best_params_, 'score': gs.best_score_, 'model': 'Random Forest', 'variables': X_under.columns}, ignore_index=True)\n",
    "\t\n",
    "\treturn results\n",
    "\n",
    "def random_forest_time(X_under, y_under, results, options):\n",
    "\tX_filtered_sorted = X_under.sort_index(key=lambda d: d.map(lambda x: x[1]))\n",
    "\ty_under_sorted = y_under.sort_index(key=lambda d: d.map(lambda x: x[1]))\n",
    "\ttscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "\tparam_grid = {\n",
    "\t\t'max_depth': (1, 5, 10, 25),\n",
    "\t\t'n_estimators': (100, 500, 750, 1500),\n",
    "\t\t'max_features': (2, 3, 5, 10)}\n",
    "\n",
    "\tclf = RandomForestClassifier(n_jobs=-1, random_state=42, verbose=0)\n",
    "\n",
    "\tgs = GridSearchCV(clf, param_grid=param_grid, cv=tscv,\n",
    "\t\t\t\t\tscoring=auc_pr, n_jobs=-1, verbose=0)\n",
    "\tgs.fit(X_filtered_sorted, y_under_sorted)\n",
    "\t\n",
    "\tresults = results.append({'n_days': N_DAYS, 'time_lag': TIME_LAG, 'eval_type': 'time', 'param_type': options['param_type'], 'year': options['year'], \n",
    "                           'params': gs.best_params_, 'score': gs.best_score_, 'model': 'Random Forest', 'variables': X_under.columns}, ignore_index=True)\n",
    "\tprint(gs.best_params_, gs.best_score_)\n",
    "\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def gradient_boost_site(X_under, y_under, results, options):\n",
    "\tindx = X_under.index.unique()\n",
    "\tsites = [x[0] for x in indx]\n",
    "\tsites = list(set(sites))\n",
    "\tsites_train = sites[:len(sites)//10*6]\n",
    "\tsites_test = sites[len(sites)//10*6:]\n",
    "\tidx_train = [x for x in indx if x[0] in sites_train]\n",
    "\tidx_test = [x for x in indx if x[0] in sites_test]\n",
    "\tX_filtered_train = X_under[X_under.index.isin(idx_train)]\n",
    "\tX_filtered_test = X_under[X_under.index.isin(idx_test)]\n",
    "\ty_train = y_under[y_under.index.isin(idx_train)]\n",
    "\ty_test = y_under[y_under.index.isin(idx_test)]\n",
    "\n",
    "\tX_all = pd.concat([X_filtered_train, X_filtered_test]).reset_index(drop=True)\n",
    "\ty_all = pd.concat([y_train, y_test]).reset_index(drop=True)\n",
    "\tclf = HistGradientBoostingClassifier(random_state=42, verbose=0, early_stopping=False)\n",
    "\n",
    "\tsplit_index = [-1 if x in X_filtered_train.index else 0 for x in X_under.index]\n",
    "\tps = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "\tparam_grid = {'max_iter': (100, 1000, 1500),\n",
    "               'learning_rate': (0.01, 0.1, 1),\n",
    "               'max_depth': (1, 5, 10, 25, 50),\n",
    "               }\n",
    "\n",
    "\tgs = GridSearchCV(clf, param_grid=param_grid, cv=ps,\n",
    "                   scoring=auc_pr, n_jobs=-1, verbose=0)\n",
    "\tgs.fit(X_all, y_all)\n",
    "\tprint(gs.best_params_, gs.best_score_)\n",
    "\tresults = results.append({'n_days': N_DAYS, 'time_lag': TIME_LAG, 'eval_type': 'site', 'param_type': options['param_type'], 'year': options['year'],\n",
    "                           'params': gs.best_params_, 'score': gs.best_score_, 'model': 'Gradient Boost', 'variables': X_under.columns}, ignore_index=True)\n",
    "\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def gradient_boost_time(X_under, y_under, results, options):\n",
    "\tX_filtered_sorted = X_under.sort_index(key=lambda d: d.map(lambda x: x[1]))\n",
    "\ty_under_sorted = y_under.sort_index(key=lambda d: d.map(lambda x: x[1]))\n",
    "\ttscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "\tparam_grid = {'max_iter': (100, 1000, 1500),\n",
    "               'learning_rate': (0.01, 0.1, 1),\n",
    "               'max_depth': (1, 5, 10, 25, 50),\n",
    "               }\n",
    "\tclf = HistGradientBoostingClassifier(random_state=42, verbose=0, early_stopping=False)\n",
    "\n",
    "\tgs = GridSearchCV(clf, param_grid=param_grid, cv=tscv,\n",
    "                   scoring=auc_pr, n_jobs=-1, verbose=0)\n",
    "\tgs.fit(X_filtered_sorted, y_under_sorted)\n",
    "\n",
    "\tresults = results.append({'n_days': N_DAYS, 'time_lag': TIME_LAG, 'eval_type': 'time', 'param_type': options['param_type'], 'year': options['year'],\n",
    "                           'params': gs.best_params_, 'score': gs.best_score_, 'model': 'Gradient Boost', 'variables': X_under.columns}, ignore_index=True)\n",
    "\tprint(gs.best_params_, gs.best_score_)\n",
    "\n",
    "\treturn results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create rolled dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_clean = pd.read_csv('../all_data_clean.csv')\n",
    "\n",
    "all_data_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Here can change parameters only once\n",
    "df_rolled = roll_time_series(\n",
    "    all_data_clean[['date', 'flow_site_id', 'flow', 'temp', 'prec', 'binary']], column_id=\"flow_site_id\", column_sort=\"date\", max_timeshift=N_DAYS, min_timeshift=N_DAYS - 1, n_jobs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled.to_csv('../df_rolled_' + str(N_DAYS) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract minimal timeseries features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled = pd.read_csv('../df_rolled_' + str(N_DAYS) + '.csv')\n",
    "all_data_clean = pd.read_csv('../all_data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract timeseries features\n",
    "\n",
    "X_features_all = extract_features(\n",
    "\tdf_rolled.drop([\"binary\", \"flow_site_id\"], axis=1), column_id='id', column_sort='date',\n",
    "\tn_jobs=20, disable_progressbar=False, default_fc_parameters=MinimalFCParameters())\n",
    "\n",
    "\n",
    "X_features_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add binary response variable back based on unique id\n",
    "\n",
    "X_features_all['unique_id'] = X_features_all.index\n",
    "X_features_all['unique_id'] = X_features_all['unique_id'].apply(ast.literal_eval)\n",
    "\n",
    "all_data_clean['shifted_date'] = pd.to_datetime(\n",
    "    all_data_clean.date) + pd.Timedelta(days=TIME_LAG)\n",
    "all_data_clean['shifted_date'] = all_data_clean['shifted_date'].dt.strftime(\n",
    "    '%Y-%m-%d')\n",
    "all_data_clean['unique_id'] = list(\n",
    "    zip(all_data_clean.flow_site_id, all_data_clean.shifted_date))\n",
    "all_data_clean = all_data_clean.dropna()\n",
    "\n",
    "X_features_all = X_features_all.reset_index(drop=True)\n",
    "\t\n",
    "X_features_all = pd.merge(X_features_all, all_data_clean[[\n",
    "                          'binary', 'unique_id']], how='left', on='unique_id')\n",
    "X_features_all = X_features_all.set_index(\n",
    "    X_features_all['unique_id'], drop=True)\n",
    "X_features_all = X_features_all.dropna()\n",
    "X_features_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_all.to_csv('../df_extracted_min_' + str(N_DAYS) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersample minimal timeseries feature dataset and run Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_all = pd.read_csv('../df_extracted_min_' + str(N_DAYS) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_all['unique_id'] = X_features_all['unique_id'].apply(ast.literal_eval)\n",
    "X_features_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = X_features_all['binary']\n",
    "Counter(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## undersample\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "undersample = NearMiss(version=3, n_neighbors=3)\n",
    "X_under, y_under = undersample.fit_resample(\n",
    "    X_features_all.drop(columns=['binary', 'unique_id']), y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_under.index = X_features_all['unique_id'][undersample.sample_indices_]\n",
    "y_under.index = X_features_all['unique_id'][undersample.sample_indices_]\n",
    "Counter(y_under)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['n_days', 'time_lag', 'eval_type', 'param_type', 'year', 'params', 'score', 'model', 'variables'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for random forest\n",
    "sel = SelectFromModel(RandomForestClassifier(n_jobs=-1, random_state=42))\n",
    "sel.fit(X_under, y_under)\n",
    "selected_feat = X_under.columns[(sel.get_support())]\n",
    "X_selected = X_under[selected_feat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlated features\n",
    "corr_matrix = X_selected.corr().abs()\n",
    "upper = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "X_selected.drop(to_drop, axis=1, inplace=True)\n",
    "X_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model without year and split by site and time\n",
    "results = random_forest_site(X_selected, y_under, results, options={'param_type': 'Minimal', 'year': 'No'})\n",
    "results = random_forest_time(X_selected, y_under, results, options={'param_type': 'Minimal', 'year': 'No'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model without year and split by site and time\n",
    "results = gradient_boost_site(X_selected, y_under, results, options={'param_type': 'Minimal', 'year': 'No'})\n",
    "results = gradient_boost_time(X_selected, y_under, results, options={'param_type': 'Minimal', 'year': 'No'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add year to features\n",
    "dates = [parser.parse(x[1]).year for x in X_selected.index]\n",
    "X_selected['year'] = dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model with year and split by site and time\n",
    "results = random_forest_site(X_selected, y_under, results, options={'param_type': 'Minimal', 'year': 'Yes'})\n",
    "results = random_forest_time(X_selected, y_under, results, options={'param_type': 'Minimal', 'year': 'Yes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model with year and split by site and time\n",
    "results = gradient_boost_site(X_selected, y_under, results, options={'param_type': 'Minimal', 'year': 'Yes'})\n",
    "results = gradient_boost_time(X_selected, y_under, results, options={'param_type': 'Minimal', 'year': 'Yes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled['id'] = df_rolled['id'].apply(ast.literal_eval)\n",
    "df_rolled.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_under_all = df_rolled[df_rolled.id.isin(X_under.index)]\n",
    "X_under_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_under_all.to_csv('../df_undersampled_nearmiss_' +\n",
    "                   str(N_DAYS) + '_time_lag_' + str(TIME_LAG) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract complete set of timeseries features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_under_all = pd.read_csv('../df_undersampled_nearmiss_' +\n",
    "                          str(N_DAYS) + '_time_lag_' + str(TIME_LAG) + '.csv')\n",
    "X_under_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract timeseries features\n",
    "\n",
    "X_features_all = extract_features(\n",
    "\tX_under_all.drop([\"binary\", \"flow_site_id\"], axis=1), column_id='id', column_sort='date',\n",
    "\tn_jobs=20, disable_progressbar=False)\n",
    "\n",
    "\n",
    "X_features_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_all = X_features_all.dropna(axis=1)\n",
    "X_features_all['unique_id'] = X_features_all.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_all.to_csv(\n",
    "    '../df_extracted_all_nearmiss_' + str(N_DAYS) + '_time_lag_' + str(TIME_LAG) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Random Forest model with complete set of extracted timeseries parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_all = pd.read_csv(\n",
    "    '../df_extracted_all_nearmiss_' + str(N_DAYS) + '_time_lag_' + str(TIME_LAG) + '.csv')\n",
    "\n",
    "all_data_clean = pd.read_csv('../all_data_clean.csv')\n",
    "\n",
    "### to get the binary labels for n days after end of rolled time series\n",
    "all_data_clean['shifted_date'] = pd.to_datetime(\n",
    "    all_data_clean.date) + pd.Timedelta(days=TIME_LAG)\n",
    "all_data_clean['shifted_date'] = all_data_clean['shifted_date'].dt.strftime(\n",
    "    '%Y-%m-%d')\n",
    "all_data_clean['unique_id'] = list(\n",
    "    zip(all_data_clean.flow_site_id, all_data_clean.shifted_date))\n",
    "all_data_clean = all_data_clean.dropna()\n",
    "all_data_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_all['unique_id'] = X_features_all['unique_id'].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_under_all = pd.merge(X_features_all, all_data_clean[[\n",
    "    'binary', 'year', 'unique_id']], how='left', on='unique_id')\n",
    "X_features_under_all = X_features_under_all.set_index(\n",
    "    X_features_under_all['unique_id'], drop=True)\n",
    "y_under = X_features_under_all['binary']\n",
    "X_features_under_filtered  =X_features_under_all.replace(np.inf, np.nan)\n",
    "X_features_under_filtered = X_features_under_filtered.dropna(axis=1)\n",
    "X_features_under_filtered = X_features_under_filtered.drop(\n",
    "    columns=['unique_id', 'binary'])\n",
    "X_features_under_filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model without year and split by site and time\n",
    "#results = random_forest_site(X_features_under_filtered.drop(columns=['year']), y_under, results, options={'param_type': 'All', 'year': 'No'})\n",
    "#results = random_forest_time(X_features_under_filtered.drop(columns=['year']), y_under, results, options={'param_type': 'All', 'year': 'No'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model without year and split by site and time\n",
    "#results = gradient_boost_site(X_features_under_filtered.drop(columns=['year']), y_under, results, options={'param_type': 'All', 'year': 'No'})\n",
    "#results = gradient_boost_time(X_features_under_filtered.drop(columns=['year']), y_under, results, options={'param_type': 'All', 'year': 'No'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model with year and split by site and time\n",
    "#results = random_forest_site(X_features_under_filtered, y_under, results, options={'param_type': 'All', 'year': 'Yes'})\n",
    "#results = random_forest_time(X_features_under_filtered, y_under, results, options={'param_type': 'All', 'year': 'Yes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model with year and split by site and time\n",
    "#results = gradient_boost_site(X_features_under_filtered, y_under, results, options={'param_type': 'All', 'year': 'Yes'})\n",
    "#results = gradient_boost_time(X_features_under_filtered, y_under, results, options={'param_type': 'All', 'year': 'Yes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(RandomForestClassifier(n_jobs=-1, random_state=42))\n",
    "sel.fit(X_features_under_filtered.drop(columns=['year']), y_under)\n",
    "selected_feat = X_features_under_filtered.drop(\n",
    "    columns=['year']).columns[(sel.get_support())]\n",
    "X_selected = X_features_under_filtered[selected_feat]\n",
    "X_selected['year'] = X_features_under_filtered['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlated features\n",
    "corr_matrix = X_selected.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "X_selected.drop(to_drop, axis=1, inplace=True)\n",
    "X_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model without year and split by site and time\n",
    "results = random_forest_site(X_selected.drop(columns=['year']), y_under, results, options={'param_type': 'Selected', 'year': 'No'})\n",
    "results = random_forest_time(X_selected.drop(columns=['year']), y_under, results, options={'param_type': 'Selected', 'year': 'No'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model without year and split by site and time\n",
    "results = gradient_boost_site(X_selected.drop(columns=['year']), y_under, results, options={'param_type': 'Selected', 'year': 'No'})\n",
    "results = gradient_boost_time(X_selected.drop(columns=['year']), y_under, results, options={'param_type': 'Selected', 'year': 'No'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model with year and split by site and time\n",
    "results = random_forest_site(X_selected, y_under, results, options={'param_type': 'Selected', 'year': 'Yes'})\n",
    "results = random_forest_time(X_selected, y_under, results, options={'param_type': 'Selected', 'year': 'Yes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run model with year and split by site and time\n",
    "results = gradient_boost_site(X_selected, y_under, results, options={'param_type': 'Selected', 'year': 'Yes'})\n",
    "results = gradient_boost_time(X_selected, y_under, results, options={'param_type': 'Selected', 'year': 'Yes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../results_' + str(N_DAYS) + '_time_lag_' + str(TIME_LAG) + '_flow_temp_prec.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
